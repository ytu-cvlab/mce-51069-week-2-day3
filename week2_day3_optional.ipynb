{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"2020-12-21-week2-day3_optional_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7cfa2a512265471981012eb684dfc822":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8a9f0f12963e463c88b31e3233dce586","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cd4143d454ba4a579bcc2d1e9b344ca0","IPY_MODEL_3b29eb7ec4c74bc0ba997cc1a1170b4f"]}},"8a9f0f12963e463c88b31e3233dce586":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cd4143d454ba4a579bcc2d1e9b344ca0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1f28769831294a4fb407b3bd3fa4eb0c","_dom_classes":[],"description":" 95%","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c726b1027a354e66a06fb3a04a5f7ed1"}},"3b29eb7ec4c74bc0ba997cc1a1170b4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_973499ff2d344a9f8ab8db72b5655051","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 162193408/170498071 [00:01&lt;00:00, 95393129.21it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0bebddaf1d914fe9b0c9afecbc18cddb"}},"1f28769831294a4fb407b3bd3fa4eb0c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c726b1027a354e66a06fb3a04a5f7ed1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"973499ff2d344a9f8ab8db72b5655051":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0bebddaf1d914fe9b0c9afecbc18cddb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"UyFbiPnzt8pB"},"source":["# Week 2, Day 3 (Guided Project with Pytorch on Image Classification)\n","> Welcome to third day (Week 2) of the McE-51069 course. We will walk you through with the basic flow of building the Convolutional Neural Network.\n","\n","- sticky_rank: 5\n","- toc: true\n","- badges: true\n","- comments: false\n","- categories: [Pytorch, Convolutional_Neural_Network]"]},{"cell_type":"markdown","metadata":{"id":"9eqjw6IfFH_G"},"source":["# Assignment\n","\n","For the assignment of Week 2, Please visit this [link](http://colab.research.google.com/github/ytu-cvlab/mce-51069-week-2-day3/blob/main/Assignment_3.ipynb) and copy the notebook to your google drive.\n","\n","After training the network for MNIST dataset is finised, please submit the weight file save at the last cell to this [link]."]},{"cell_type":"markdown","metadata":{"id":"KgJGtoDugBEx"},"source":["# Import Necessary Libraries\n","\n","Pytorch is the open source machine learning framework that we can use for research to production.\n","\n","If you would like to study the tutorials from the offical [pytorch](https://pytorch.org/) website, please visit [this link](https://pytorch.org/tutorials/). The source code for the entire pytorch framework can be found [here](https://github.com/pytorch).\n","\n","> SideNote: Pytorch separates different tasks into different modules. e.g., there is a package called **torchaudio** that focus only on audio alone.\n","\n","\n","Today we will use `torchvision`, which is the package inside torch, that focus on vision tasks. For example, for data augmentation, torchvision provides the function called **transforms**. And if you would like to use transfer learning, torchvision also provides some of the state of the art pretrained models.\n","\n","The `torch.nn` contains the basic building blocks that we need to construct our model. For example, if we need to construct a **Convolutional Layer**, we can call the function `torch.nn.Conv2d()` to construct that layer.\n","\n","And if we want linear layer that perform the equation of $y = W^TX + b$, we can call the function `torch.nn.Linear()`.\n","\n","If you would like to know more about `torch.nn` library, please visit this [link](https://pytorch.org/docs/stable/nn.html) for more information. Also, this [post](https://pytorch.org/tutorials/beginner/nn_tutorial.html) provides better understanding of `torch.nn` module."]},{"cell_type":"code","metadata":{"id":"wejqfkizlvnD","executionInfo":{"status":"ok","timestamp":1608804619957,"user_tz":-390,"elapsed":5270,"user":{"displayName":"Aung Paing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64","userId":"03851297839959231119"}}},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_uDlHmO-pCWH"},"source":["# Main Components\n","\n","Let's preview what will be built in this notebook.\n","\n"," 1. Dataset \n","  - cifar10\n"," 2. Model Architecture\n","  - Simple Model\n"," 3. Loss (Update model)\n","  - Cross Entropy\n"," 4. Optimizer (Regularizer)\n","  - Adam Optimizer\n"," 5. Metrics (Visual for User)\n","  - Loss, Accuracy\n"," 6. Save Model\n","  - Model Checkpoint"]},{"cell_type":"markdown","metadata":{"id":"Zmfg8ySjn0D1"},"source":["If you would like to change from CPU to GPU,\n","select the `Runtime --> Change Runtime type` and select `GPU`.\n","\n","After done selecting, we can check whether we are running GPU or CPU by using the following function:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SSDu6fd4t8pW","executionInfo":{"status":"ok","timestamp":1608804619959,"user_tz":-390,"elapsed":5203,"user":{"displayName":"Aung Paing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64","userId":"03851297839959231119"}},"outputId":"2a1e92f3-0b90-446d-a09c-2a74cf50124c"},"source":["# Check if GPU is available\n","print(torch.cuda.is_available())"],"execution_count":2,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xfhNywLHWTco","executionInfo":{"status":"ok","timestamp":1608804619960,"user_tz":-390,"elapsed":5126,"user":{"displayName":"Aung Paing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64","userId":"03851297839959231119"}},"outputId":"c5880e0a-c9bf-49f6-c7dd-b04ce6aceacb"},"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xP_JtqqTt8pY"},"source":["If the output show `cuda:0`, then the `GPU` is in used."]},{"cell_type":"markdown","metadata":{"id":"6Bvkx_6YoXM0"},"source":["# Dataset\n","\n","In this notebook, we will use `CIFAR10` dataset for classification. \n","\n","[CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) is a public classification dataset, which consists of 60,000 32 * 32 color images in 10 classes. There are 50,000 training images and 10,000 testing images. \n","\n","In the following cell, we will do data augmentation for the dataset. When doing **trasnforms** on the test dataset, we only *normalize* the input, that is because we do not need to augment(change) our test dataset to see the result.\n","\n","Tensor is just like Numpy's ndarray, except that it can do calculations on GPU, and is modified to fit the training neural network procedure.\n","\n","Normalization equation:\n","\n","$X = \\frac{X-\\mu}{\\sigma}$.\n","\n","There are many augmentation methods available, if you would like to know more, please visit this [post](https://pytorch.org/docs/stable/torchvision/transforms.html)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["7cfa2a512265471981012eb684dfc822","8a9f0f12963e463c88b31e3233dce586","cd4143d454ba4a579bcc2d1e9b344ca0","3b29eb7ec4c74bc0ba997cc1a1170b4f","1f28769831294a4fb407b3bd3fa4eb0c","c726b1027a354e66a06fb3a04a5f7ed1","973499ff2d344a9f8ab8db72b5655051","0bebddaf1d914fe9b0c9afecbc18cddb"]},"id":"QOOHZDMPo1UV","outputId":"80e75a12-7a5b-4e16-a764-45b768314020"},"source":["# The class names for CIFAR 10\n","class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n","               'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","\n","# Image Normalization, Data Augmentation\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((0.5,), (0.5,)),\n","                                transforms.RandomRotation(30)\n","                                ])\n","\n","test_transform = transforms.Compose([\n","                                     transforms.ToTensor(),\n","                                     transforms.Normalize((0.5,), (0.5,))\n","])\n","train_dataset = torchvision.datasets.CIFAR10(\n","        './data', train=True,\n","        transform=transform, download=True)\n","\n","test_dataset = torchvision.datasets.CIFAR10(\n","        './data', train=False,\n","        transform=test_transform, download=True\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7cfa2a512265471981012eb684dfc822","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"afcYs8kvvkEu"},"source":["> Note: Pytorch stores the image dataset in the following format:\n","`batch_size * dim * height * width (B*D*H*W)`.\n","\n","`batch_size` means number of images for one training iteration.\n","\n","`dim` means the image dimension. Conventionally, color images have the dimension of *3* and gray scale images have the dimension of *1*.\n","\n","The label value for the dataset ranges from `0~9`. e.g., if the label is *7*, then, it would be **class_names[7] = horse**."]},{"cell_type":"markdown","metadata":{"id":"xU-e7QXcx1Jp"},"source":["## Dataset and DataLoader\n","\n","Dataset plays the huge role in machine learning and deep learning. For the computer vision task, we can do data augmentation to get the effect of regulating the model, avoid being overfitting. And when training, the augmented dataset needs to be fetched by the data loader. The main duty of the dataloader is to prepare dataset before feeding into the neural network.\n","\n","Dataloader object is a generator object, which can be accessed through iteration."]},{"cell_type":"code","metadata":{"id":"fl5zipzpsfMO"},"source":["train_loader = torch.utils.data.DataLoader(train_dataset, 32, shuffle=True,\n","                                           num_workers=2)\n","\n","test_loader = torch.utils.data.DataLoader(test_dataset, 8, shuffle=True,\n","                                           num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQU-uJSh2xu4"},"source":["# Model\n","\n","Let's build a simple Convolutional Neural Network.\n","\n","When flattening the network, we can use the equation:\n","\n","$O = \\frac{W-K+2P}{S} + 1$"]},{"cell_type":"code","metadata":{"id":"5PAj_NegsfOi"},"source":["class SampleModel(nn.Module):\n","    def __init__(self):\n","        super(SampleModel, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, 3)\n","        self.conv2 = nn.Conv2d(64, 256, 3)\n","        self.conv3 = nn.Conv2d(256, 256, 3)\n","        self.conv4 = nn.Conv2d(256, 128, 3)\n","        self.fc1 = nn.Linear(24*24*128, 512)\n","        self.fc2 = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","        x = x.view(-1, 24*24*128)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYI-5pmqsfQ8"},"source":["model = SampleModel()\n","\n","# Move model to GPU\n","model.to(device)\n","\n","\n","# Define Loss\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define Optimizer\n","optimizer = optim.Adam(model.parameters())\n","# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rf9G98RAZAF2"},"source":["# Let's Inspect our model.\n","\n","We can get the weights of the model by two methods: `model.parameters()` and `model.state_dict()`.\n","\n","`model.parameters()` contain the value of the weight. For example, in Conv2d layer, it will be the kernel value.\n","\n","`model.state_dict()` is a dictionary object which have key as the parameter name and value of model.parameters()."]},{"cell_type":"code","metadata":{"id":"dtk2qj7HY_HI"},"source":["print(type(model.state_dict()))\n","print(type(model.parameters()))\n","print()\n","\n","for para in model.parameters():\n","    print(para.size())\n","    break\n","\n","for stats in model.state_dict():\n","    print(f\"Key : {stats}, Value : {model.state_dict()[stats].size()}\")\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-inEks5rbWhp"},"source":["# We could also write the above with key manually provided.\n","first_conv = model.state_dict()['conv1.weight']\n","print(first_conv.size())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S24XhmrXadGN"},"source":["# Helper function for convolution in 3D image\n","def conv2d_cv2(img, filter):\n","    h, w, d = img.shape\n","    filtered = np.zeros_like(img)\n","    for i in range(3):\n","        img_ = img[:,:,i]\n","        filter_ = filter[:,:,i]\n","        result = cv2.filter2D(img_, -1, filter_)\n","        filtered[:,:,i] = result\n","\n","    filtered = filtered.mean(axis=2)\n","    return filtered\n","\n","# Helper function\n","def visualize_filters(model=model):\n","    '''\n","    Visualize Convolutional layer (Weight of model).\n","    '''\n","    first_conv = model.state_dict()['conv1.weight']\n","    plt.figure(figsize=(20, 20))\n","    for i, filter in enumerate(first_conv):\n","        filter = filter.cpu().numpy().transpose(1, 2, 0)\n","\n","        filter += np.abs(np.min(filter))\n","        filter /= np.max(filter)\n","        plt.subplot(8, 8, i+1)\n","        plt.imshow(filter)\n","        # print(np.max(filter), np.min(filter))\n","        # break\n","    plt.show()\n","\n","# Helper function to visualize first layer output\n","def visualize_model(model=model, img_ind = 10):\n","    '''\n","    Visualize model for better understanding.\n","    Args:\n","        model -> model trained or not trained.\n","    '''\n","    image = train_dataset[img_ind][0].cpu().numpy().transpose(1, 2, 0)\n","    first_conv = model.state_dict()['conv1.weight']\n","    plt.figure(figsize=(20, 20))\n","    for i, filter in enumerate(first_conv):\n","        filter = filter.cpu().numpy().transpose(1, 2, 0)\n","        filtered = conv2d_cv2(image, filter)\n","        plt.subplot(8, 8, i+1)\n","        plt.imshow(filtered, cmap='gray')\n","\n","    plt.show()\n","\n","\n","visualize_filters(model)\n","visualize_model(model, img_ind=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9y8EErl8Vqf"},"source":["# Train and plot model performance\n","EPOCH = 10\n","\n","loss_log = []\n","acc_log = []\n","x_coor = []\n","\n","for epoch in range(EPOCH):\n","    loss_epoch = 0\n","    total_imgs = 0\n","    correct_epoch = 0\n","\n","    for i, data in enumerate(train_loader):\n","        \n","        # Get Image data and Label data\n","        imgs, labels = data[0].to(device), data[1].to(device)\n","\n","        # Clear out the gradient\n","        optimizer.zero_grad()\n","\n","\n","        # Forward Propagation Start\n","        # Predict from Input : B * 10\n","        predicts = model(imgs)\n","\n","        # Calculate Loss from batch : 1\n","        loss = criterion(predicts, labels)\n","        # Forward Propagation End\n","\n","\n","        # Backward Propagation Start\n","        # Calculate Gradient\n","        loss.backward()\n","\n","        # Update Model parameters with optimizer : Adam or SGD\n","        optimizer.step()\n","        # Backward Propagation End\n","\n","\n","        # Add to Epoch Loss\n","        loss_epoch += loss.item()\n","        # Total Number of images in one batch\n","        total_imgs += len(imgs)\n","        # Count the total number of correct prediction\n","        correct_batch = (torch.argmax(predicts, 1)==labels).sum().item()\n","        correct_epoch += correct_batch\n","        acc_batch = correct_batch/ len(imgs)\n","\n","        # Adding to tensorboard\n","        x_coor.append((i*len(imgs))+(epoch*len(train_dataset)))\n","        loss_log.append(loss.item())\n","        acc_log.append(acc_batch)\n","\n","\n","    acc_epoch = (correct_epoch/total_imgs)*100\n","    loss_epoch = loss_epoch/total_imgs\n","    print(f\"EPOCH : {epoch+1}, Acc : {acc_epoch:.2f}, Loss : {loss_epoch:.2f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C_noCBg2hAgJ"},"source":["# Let's visualize our model filter again\n","visualize_filters(model)\n","visualize_model(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jJTs4WiGt8pl"},"source":["# Saving Model\n","\n","When saving our model, we often save the model `state_dict`. The saved model file contain the values for the weight and the variable name indicating that weight.\n","\n","So, if we try to reload the model next time, the file will automatically look for the same variable name."]},{"cell_type":"code","metadata":{"id":"Hf6RjLLV6T5P"},"source":["# Checkpoint\n","ckpt_path = 'checkpoint.pt'\n","\n","# State Difference between model.parameters and model.state_dict()\n","torch.save(model.state_dict(), ckpt_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"thAarnq5azZQ"},"source":["saved_model = SampleModel().to(device)\n","checkpoint = torch.load(ckpt_path)\n","saved_model.load_state_dict(checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m9tiqgIiiqbr"},"source":["# Test Dataset accuracy:\n","num_total = 0\n","num_correct = 0\n","\n","# Deactivate Drop out and Batch-Normalization layers.\n","model.eval()\n","\n","# Do not store gradient info in forward propagation.\n","with torch.no_grad():\n","    for i, data in enumerate(test_loader):\n","        image, label = data\n","        image = image.to(device)\n","        label = label.to(device)\n","\n","        predict = model(image)\n","        predict_class = torch.argmax(predict, dim=1)\n","        correct = (predict_class == label)\n","\n","\n","        num_correct += correct.sum().item()\n","        num_total += len(image)\n","        \n","print(num_correct, num_total)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSVOWVrviqh5"},"source":["print(\"Accuracy : \", num_correct/num_total)"],"execution_count":null,"outputs":[]}]}